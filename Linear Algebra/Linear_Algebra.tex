%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LaTeX book template                           %%
%% Author:  Amber Jain (http://amberj.devio.us/) %%
%% License: ISC license                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Source: http://en.wikibooks.org/wiki/LaTeX/Hyperlinks %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{/Users/HechenHu/Development/NoteTaking/Mathematics-Notes/Customized}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter quote at the start of chapter        %
% Source: http://tex.stackexchange.com/a/53380 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makeatletter

\renewcommand{\@chapapp}{}% Not necessary...

\newenvironment{chapquote}[2][2em]
{\setlength{\@tempdima}{#1}%
	\def\chapquote@author{#2}%
	\parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
	\itshape}
{\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First page of book which contains 'stuff' like: %
%  - Book title, subtitle                         %
%  - Book author name                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Book's title and subtitle
\title{\Huge \textbf{Linear Algebra}}
% Author
\author{\textsc{Hechen Hu}}

\begin{document}
	\frontmatter
	\maketitle
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% Auto-generated table of contents, list of figures and list of tables %
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\tableofcontents
	
	\mainmatter
	
	\chapter{Solving Linear Equations}
	\section{Systems of Linear Equations}
	\begin{Definition}
		A \textit{linear equation} in the variables $ x_1,\cdots,x_n $ is an equation that can be written in the form
		\begin{equation}
			a_1 x_1 + a_2 x_2 + \cdots + a_n x_n = b \nonumber
		\end{equation}
		where $ b $ and coefficients $ a_i $ are real or complex numbers. A \textit{linear system} is a collection of one or more linear equations involving the same variables. A \textit{solution} of the system is a list of numbers that makes each equation a true statement when their values are substituted for $ x_1,\cdots,x_n $ respectively. The set of all possible solutions is called the \textit{solution set} of the linear system. Two linear systems are called \textit{equivalent} if they have the same solution set.
	\end{Definition}
\begin{Definition}
	A linear system is \textit{consistent} if it has either one solution or infinitely many solutions. If it has no solution, it is called \textit{inconsistent}.
\end{Definition}
\begin{Definition}
	The \textit{coefficient matrix} is the matrix where the coefficients of each variable in a system aligned in columns. If additionally the coefficient of the right-hand side of equations are added to the coefficient matrix, a new matrix called \textit{augmented matrix} is generated.
\end{Definition}
\begin{Definition}
\textit{Elementary row operations} on a matrix include:
\begin{itemize}
	\item (\textit{Replacement}) Replace one row by the sum of itself and a multiple of another row.
	\item (\textit{Interchange}) Interchange two rows.
	\item (\textit{Scaling}) Multiply all entries in a row by a nonzero constant.
\end{itemize}
Two matrices are \textit{row equivalent} if there is a sequence of elementary operations that transforms one matrix into the other.
\end{Definition}
\begin{Theorem}
	If the augmented matrices of two linear systems are row equivalent, then the two systems have the same solution set.
\end{Theorem}
\section{Row Reduction and Echelon Forms}
\begin{Definition}
	A rectangular matrix is in \textit{echelon form} if it has the following properties:
	\begin{itemize}
		\item All nonzero rows are above any rows of all zeros;
		\item Each leading entry of a row is in a column to the right of the leading entry of the row above it;
		\item All entries in a column below a leading entry are zero.
	\end{itemize}
If a matrix in echelon form satisfies the following additional conditions, then it is in \textit{reduced echelon form}:
\begin{itemize}
	\item The leading entry in each nonzero row is $ 1 $;
	\item Each leading $ 1 $ is the only nonzero entry in its column.
\end{itemize}
\end{Definition}
\begin{Theorem}
	Each matrix is row equivalent to an unique reduced echelon matrix.
\end{Theorem}
If a matrix $ A $ is row equivalent to an (reduced)echelon matrix $ U $, $ U $ is called an \textit{(reduced) echelon form of $ A $}. The abbreviation RREF and REF are used for reduced (row) echelon form and (row) echelon form respectively.
\begin{Definition}
	A \textit{pivot position} in a matrix $ A $ is a location in $ A $ that corresponds to a leading entry in an echelon form of $ A $. A \textit{pivot column} is a column of $ A $ that contains a pivot position.
\end{Definition}
\begin{Theorem}
	A linear system is consistent iff the rightmost column of the augmented matrix \textbf{is not} a pivot column, that is, iff an echelon form of the augmented matrix has \textbf{no} row of the form
	\begin{equation}
		\begin{bmatrix}
		0 &\cdots &0 &b
		\end{bmatrix}\qquad \text{ with $ b $ nonzero}
		\nonumber
	\end{equation}
	If a linear system is consistent, then the solution set contains either
	\begin{itemize}
		\item a unique solution, when there are no free variables.
		\item infinitely many solutions, when there is at least one free variable.
	\end{itemize}
	
\end{Theorem}

\section{Vector Equations}
\begin{Definition}
	A matrix with only one column is called a \textit{column vector}, or simply a \textit{vector}.
\end{Definition}
\begin{Definition}
	Given vectors $ \vec{v}_1,\vec{v}_2,\cdots,\vec{v}_p $ in $ \Real^n $ and given scalars $ c_1,c_2,\cdots,c_p $, the vector $ \vec{y} $ defined by
	\begin{equation}
		\vec{y}=c_1 \vec{v}_1+ \cdots + c_p \vec{v}_p \nonumber
	\end{equation}
	is called a \textit{linear combination of $ \vec{v}_1,\vec{v}_2,\cdots,\vec{v}_p  $ using weights $ c_1,c_2,\cdots,c_p $}.
\end{Definition}
\begin{Definition}
	If $ \vec{v}_1,\vec{v}_2,\cdots,\vec{v}_p $ are in $ \Real^n $ , then the set of all linear combinations of them is denoted by $ \spn \{\vec{v}_1,\cdots,\vec{v}_p\} $ and is called the \textit{subset of $ \Real^n $ spanned (or generated) by $ \vec{v}_1,\cdots,\vec{v}_p $}. That is, $ \spn \{\vec{v}_1,\cdots,\vec{v}_p\}$ is the collection of all vectors that can be written in the form
	\begin{equation}
		c_1 \vec{v}_1+ \cdots + c_p \vec{v}_p \nonumber
	\end{equation}
	with $ c_1,c_2,\cdots,c_p $ scalars.
	
\end{Definition}

\section{The Matrix Equation $ \matr{A} \vec{x} =\vec{b} $}
\begin{Definition}
	If $ \matr{A} $ is an $ m \times n $ matrix, with columns $ a_1,\cdots,a_n $, and if $ \vec{x} $ is in $ \Real^n $, then the \textit{product of $ \matr{A} $ and $ \vec{x} $}, denoted by $ \matr{A}\vec{x} $, is the \textit{linear combination of the columns of $ \matr{A} $ using the corresponding entries in $ \vec{x} $ as weights}, that is,
	\begin{equation}
		\matr{A}\vec{x} = 
		\begin{bmatrix}
		a_1 &a_2 &\cdots &a_n
		\end{bmatrix}
		\begin{bmatrix}
		x_1 \\ \cdots \\ x_n
		\end{bmatrix}
		=x_1 a_1 + x_2 a_2 + \cdots + x_n a_n \nonumber
	\end{equation}
	$ \matr{A}\vec{x} $ is defined only if the number of columns of $ \matr{A} $ equals the number of entries in $ \vec{x} $.
\end{Definition}
\begin{Definition}
	Equations having the form $ \matr{A} \vec{x} =\vec{b} $ are called \textit{matrix equations}.
\end{Definition}
\begin{Theorem}
	If $ \matr{A} $ is an $ m \times n $ matrix, with columns $ a_1,\cdots,a_n $, and $ \vec{b} $ is in $ \Real^m $, the matrix equation
	\begin{equation}
		\matr{A} \vec{x} =\vec{b} \nonumber
	\end{equation}
	has the same solution set as the vector equation
	\begin{equation}
		x_1 a_1 + x_2 a_1 + \cdots + x_n a_n = \vec{b} \nonumber
	\end{equation}
	which has the same solution set as the system of linear equations whose augmented matrix is
	\begin{equation}
			\begin{bmatrix}
		a_1 &a_2 &\cdots &a_n &\vec{b}
		\end{bmatrix}
		\nonumber
	\end{equation}
\end{Theorem}
\begin{Definition}
	A set of vectors $ \{\vec{v}_1,\cdots,\vec{v}_p \} $ in $ \Real^m $ \textit{spans (or generates)} $ \Real^m $ if $ \spn \{ \vec{v}_1,\cdots,\vec{v}_p\}=\Real^m $.
\end{Definition}
\begin{Theorem}
	Let $ \matr{A} $ be an $ m \times n $ coefficient matrix. Then the following statements are logically equivalent, that is, for a particular $ \matr{A} $, either they are all true statements or they are all false.
	\begin{itemize}
		\item For each $ \vec{b} $ in $ \Real^m $, the equation $ \matr{A} \vec{x} =\vec{b} $ has a solution.
		\item The columns of $ \matr{A} $ spans $ \Real^m $.
		\item $ \matr{A} $ has a pivot position in every row.
	\end{itemize}
\end{Theorem}
\begin{Theorem}
	If $ \matr{A} $ is an $ m \times n $ matrix, $ \vec{u} $ and $ \vec{v} $ are vectors in $ \Real^n $, and $ c $ is a scalar, then
	\begin{itemize}
		\item $ \matr{A}(\vec{u}+\vec{v}) =\matr{A}\vec{u} +\matr{A}\vec{v}$.
		\item $ \matr{A}(c \vec{u})=c ( \matr{A} \vec{u}) $.
	\end{itemize}
\end{Theorem}

\section{Solution Sets of Linear Systems}
\begin{Definition}
	A system of Linear equations is said to be \textit{homogeneous} if it can be written in the form $  \matr{A} \vec{x} =\vec{0} $. Such a system always has at least one solution, namely, $ \vec{x}=\vec{0} $, and this solution is usually called the \textit{trivial solution}. A homogeneous equation has a nontrivial solution iff the equation has at least one free variable.
\end{Definition}
\begin{Definition}
	Vector addition can be considered as a \textit{translation}. e.g. the vector $ \vec{v} $ is \textit{translated by $ \vec{p} $} to $ \vec{v}+\vec{p} $.
\end{Definition}
\begin{Definition}
	A \textit{parametric vector equation} can be written as
	\begin{equation}
		\vec{x}=s \vec{u} + t \vec{v}\qquad (s,t \in \Real) \nonumber
	\end{equation}
	which describes explicitly the spanned plane by $ \vec{u} $ and $ \vec{v} $. Whenever a solution set is described explicitly with vectors, we say that the solution is in \textit{parametric vector form}.
\end{Definition}
\begin{Theorem}
	Suppose the equation $ \matr{A} \vec{x} =\vec{b} $ is consistent for some given $ \vec{b} $, and let $ \vec{p} $ be a nonzero solution. Then the solution set of it is the set of all vectors of the form $ \vec{w}=\vec{p}+ \vec{v}_h $, where $ \vec{v_h}  $ is any solution of the homogeneous equation $ \matr{A} \vec{x} =\vec{0}  $.
\end{Theorem}

\section{Linear Independence}
\begin{Definition}
	An indexed set of vectors $ \{\vec{v}_1,\cdots,\vec{v}_p \} $ in $ \Real^n $ is said to be \textit{linearly independent} if the vector equation 
	\begin{equation}
		x_1 \vec{v}_1 + x_2 \vec{v}_2 + \cdots + x_p \vec{v}_p = \vec{0} \nonumber
	\end{equation}
	has only the trivial solution. The set $ \{\vec{v}_1,\cdots,\vec{v}_p \} $ is said to be \textit{linearly dependent} if there exist weights $ c_1,\cdots,c_p $, not all zero, such that
	\begin{equation}
		c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_p \vec{v}_p = \vec{0} \nonumber
	\end{equation}
	and this equation is called a \textit{linear dependence relation} among $ \vec{v}_1,\cdots,\vec{v}_p  $.
\end{Definition}
\begin{Theorem}
	The columns of a matrix $ \matr{A} $ are linearly independent iff the equation $ \matr{A}\vec{x}=\vec{0} $ has \textbf{only} the trivial solution.
\end{Theorem}
\begin{Theorem}
	A set of two vectors $ \{\vec{v}_1,\vec{v}_2 \} $ is linearly dependent iff one of the vectors is a multiple of the other.
\end{Theorem}
\begin{Theorem}
	An indexed set $ S= \{\vec{v}_1,\cdots,\vec{v}_p \}$ of two or more vectors is linearly dependent iff at least one of the vectors in $ S $ is a linear combination of the others.
\end{Theorem}
\begin{Theorem}
	Any set $ \{\vec{v}_1,\cdots,\vec{v}_p \} $ in $ \Real^n $ is linearly dependent if $ p>n $(Same as the criterion for the existence of solutions in a system of equations).
\end{Theorem}
\begin{Theorem}
	If a set $ S= \{\vec{v}_1,\cdots,\vec{v}_p \}$ in $ \Real^n $ contains the zero vector, then the set is linearly dependent.
\end{Theorem}

\section{Linear Transformations}
\begin{Definition}
	A \textit{transformation}(or \textit{function} or \textit{mapping}) from $ \Real^n $ to $ \Real^m $ is a rule that assigns to each vector $ \vec{x} \in \Real^n$ a vector $ T(\vec{x})\in \Real^m $. $ \Real^n $ is called the \textit{domain} of $ T $, and $ \Real^m $ is called the \textit{codomain} of $ T $. For $ \vec{x}\in \Real^n $, the vector $ T(\vec{x})\in \Real^m $ is called the \textit{image} of $ \vec{x} $ under $ T $. The set of all images $ T(\vec{x}) $ is called the \textit{range} of $ T $.
\end{Definition}
\begin{Example}
	Given a scalar $ r $, define $ T: \Real^2 \to \Real^2 $ by $ T(\vec{x})=r \vec{x} $. $ T $ is called a \textit{contraction} when $ 0 \leqslant r \leqslant 1 $ and a \textit{dilation} when $ r>1 $.
\end{Example}
\begin{Theorem}
	Let $ T: \Real^n \to \Real^m $ be a linear transformation. Then there exists a unique matrix $ \matr{A} $ such that
	\begin{equation}
		T(\vec{x})=\matr{A}\vec{x}\quad \forall \vec{x}\in \Real^n \nonumber
	\end{equation}
	In fact, $ \matr{A} $ is the $ m \times n $ matrix whose $ j $th column is the vector $ T(\vec{e}_j) $, where $ \vec{e}_j $ is the $ j $th column of the identity matrix in $ \Real^n $.
	\begin{equation}
		\matr{A}=\begin{bmatrix}
		T(\vec{e}_1) &\cdots &T(\vec{e}_n) \nonumber
		\end{bmatrix}
	\end{equation}
	The matrix $ \matr{A} $ is called the \textit{standard matrix for the linear transformation $ T $}.
\end{Theorem}
\begin{Theorem}
		Let $ T: \Real^n \to \Real^m $ be a linear transformation. Then $ T $ is injective iff the equation $ T(\vec{x})=\vec{0} $ has only the trivial solution.
\end{Theorem}
\begin{Theorem}
	Let $ T: \Real^n \to \Real^m $ be a linear transformation and let $ \matr{A} $ be the standard matrix for $ T $. Then
	\begin{itemize}
		\item $ T $ is surjective iff the columns of $ \matr{A} $ span $ \Real^m $;
		\item $ T $ is injective iff the columns of $ \matr{A} $ are linearly independent.
	\end{itemize}
\end{Theorem}
\begin{Definition}
	If there is a matrix $ \matr{A} $ such that 
	\begin{equation}
		\vec{x}_{k+1}=\matr{A} \vec{x}_k\quad \text{ for }k=0,1,2,\cdots \nonumber
	\end{equation}
	then the equation above is called a \textit{linear difference equation} (or \textit{recurrence relation}).
\end{Definition}

	\chapter{Matrices}
	\section{Matrices and Arithmetic Operations on Them}
	\begin{Definition}
		A \textit{diagonal matrix} is a square matrix whose nondiagonal entries are zero.
	\end{Definition}
\begin{Definition}
	Two matrices are equal if they have the same size and each entries are equal.
\end{Definition}
\begin{Definition}
	The \textit{sum} of two matrices is the sum of each corresponding entries in these two matrices. Thus the sum is defined only when they have the same size.
\end{Definition}
\begin{Definition}
	The \textit{scalar multiple} of a matrix has entries of the product of the scalar and each corresponding original entries.
\end{Definition}
\begin{Theorem}
	The set of matrices of the same size with respect to matrix addition and scalar multiplication over the field of real numbers is a vector space.
\end{Theorem}
\begin{Definition}
	If $ \matr{A} $ is an $ m \times n $ matrix, and if $ \matr{B} $ is an $ n \times p $ matrix with columns $ \vec{b}_1,\cdots,\vec{b}_p $, then the \textit{product} $ \matr{AB} $ is the $ m \times p $ matrix whose columns are $ \matr{A} \vec{b}_1,\cdots,\matr{A}\vec{b}_p $. Multiplication of matrices corresponds to composition of linear transformations.
\end{Definition}
\begin{Theorem}
	The multiplication has the following properties:
	\begin{itemize}
		\item Associativity of multiplication;
		\item Left distribution;
		\item Right distribution;
		\item Associativity over scalar multiplication;
		\item Identity for matrix multiplication; i.e. If $ \matr{A} $ is a matrix of size $ m \times n $, then
		\begin{equation}
			\matr{I}_m \matr{A} = \matr{A}=\matr{A}\matr{I}_n \nonumber
		\end{equation}
		where $ \matr{I}_n $ is the $ n \times n $  identity matrix.
	\end{itemize}
\end{Theorem}
\begin{Definition}
	In general, matrix multiplication is not commutative and the cancellation law do not hold. When two matrices' multiplication is commutative, they are said to be \textit{commute} with one another. Also, if a product $ \matr{A}\matr{B} $ is the zero matrix, in general it does not mean that either $ \matr{A}=\matr{0} $ or $ \matr{B}=\matr{0} $.
\end{Definition}
\begin{Definition}
	If $ \matr{A} $ is an $ m \times n $ matrix and $ k $ is a positive integer, then $ \matr{A}^k $ denoted the product of $ k $ copies of $ \matr{A} $, i.e. the $ k $th power of $ \matr{A} $. The $ 0 $th power of a matrix is the identity matrix.
\end{Definition}
\begin{Definition}
	If $ \matr{A} $ is an $ m \times n $ matrix, the \textit{transpose} of $ \matr{A} $ is the $ n \times m $ matrix, denoted $ \matr{A}^T $, whose columns are formed from the corresponding rows of $ \matr{A} $.
\end{Definition}
\begin{Theorem}
	The transpose operation has the following properties:
	\begin{itemize}
		\item $ (\matr{A}^T)^T=\matr{A} $;
		\item $ (\matr{A}+\matr{B})^T = \matr{A}^T+ \matr{B}^T $;
		\item Associativity with scalar multiplication;
		\item $ (\matr{AB})^T = \matr{B}^T \matr{A}^T $, that is, the transpose of a product of arbitrary number of matrices equals the product of their transpose in the reverse order.
	\end{itemize}
\end{Theorem}
\section{The Inverse of a Matrix}
\begin{Definition}
	If $ \matr{A} $ is an $ n \times n $ matrix, then if
	\begin{equation}
		\matr{A}\matr{A}^{-1}=\matr{I}_n\nonumber
	\end{equation}
	we say that $ \matr{A} $ is \textit{invertible} and $ \matr{A}^{-1} $ an \textit{inverse} of $ \matr{A} $. The inverse of a matrix is unique. If a matrix is not invertible, it is called a \textit{singular matrix}.
\end{Definition}
\begin{Definition}
	The \textit{determinant} of the matrix $ \matr{A} $
	\begin{equation}
		\begin{bmatrix}
		a &b \\
		c &d
		\end{bmatrix} \nonumber
	\end{equation}
	denoted $ \det \matr{A} $ and equals $ ad-bc $. Determinant is only defined for a square matrix, but the procedure above can be repeated on higher dimension matrices, for example
	\begin{equation}
		\det {\begin{bmatrix}a &b &c &d \\ e &f &g &h \\ i &j &k &l \\ m &n &o &p \end{bmatrix}}
	=a
	\det{\begin{bmatrix}f &g &h \\ j &k &l \\ n &o &p \end{bmatrix}}
	-b
	\det {\begin{bmatrix}e &g &h \\ i &k &l \\ m &o &p \end{bmatrix}}
	+c
	\det {\begin{bmatrix}e &f &h \\ i &j &l \\ m &n &p \end{bmatrix}}
	-d
	\det {\begin{bmatrix}e &f &g \\ i &j &k \\ m &n &o \end{bmatrix}} \nonumber
	\end{equation}
	By the Leibniz formula for the determinant of an $ n \times n $ matrix $ \matr{A} $ is
	\begin{equation}
		\det(\matr{A})= \sum_{\sigma \in S_n}(\sgn(\sigma)\prod_{i=1}^{n}a_{i,\sigma_{i}}) \nonumber
	\end{equation}
	Here the sum is computed over all permutations $ \sigma $ of the set $ \{1, 2, â€¦, n\} $. \\
	A permutation is a function that reorders this set of integers. The value in the $ i $th position after the reordering $ \sigma $ is denoted by $ \sigma_i $. For example, for $ n = 3 $, the original sequence $ 1, 2, 3 $ might be reordered to $ \sigma $ = $ [2, 3, 1] $, with $ \sigma_1  =  2 $, $ \sigma_2  = 3$, and $ \sigma_3  = 1$. The set of all such permutations (also known as the symmetric group on $ n $ elements) is denoted by $ S_n $. \\
	For each permutation $ \sigma $, $ \sgn(\sigma) $ denotes the signature of $ \sigma $, a value that is $ +1  $ whenever the reordering given by $ \sigma $ can be achieved by successively interchanging two entries an even number of times, and $ -1 $ whenever it can be achieved by an odd number of such interchanges.
	
\end{Definition}
\begin{Definition}
	If $ \matr{A} $ is a square matrix, then the \textit{minor} of the entry in the $ i $-th row and $ j $-th column (also called the \textit{$ (i,j) $ minor}, or a \textit{first minor}) is the determinant of the submatrix formed by deleting the $ i $-th row and $ j $-th column. This number is often denoted $ M{i,j} $. The $ (i,j) $ cofactor is obtained by multiplying the minor by $ (-1)^{i+j} $. \newpara
	In general, let A be an $ m \times n $ matrix and $ k $ an integer with $ 0 < k \leqslant m $, and $ k \leqslant n $. A $ k \times k $ minor of $ \matr{A} $, also called minor determinant of order $ k $ of $ \matr{A} $ or, if $ m=n $, $ (n-k) $th minor determinant of $  \matr{A} $, is the determinant of a $ k \times k $ matrix obtained from $ \matr{A} $ by deleting $ m - k $ rows and $ n - k $ columns. 
\end{Definition}
\begin{Definition}
	The matrix formed by all of the cofactors of a square matrix A is called the \textit{cofactor matrix}.
\end{Definition}
\begin{Definition}
	The \textit{adjugate} is the transpose of the cofactor matrix of it, that is, if $ \matr{A} $ is a matrix and $ \matr{C} $ is its cofactor matrix, then
	\begin{equation}
		\adj(\matr{A})=\matr{C}^T \nonumber
	\end{equation}
\end{Definition}
\begin{Theorem}
	A matrix $ \matr{A} $ is invertible only if $ \det(\matr{A}) \neq 0$, and in this case
	\begin{equation}
		\matr{A}^{-1}=\frac{1}{\det(\matr{A})} \adj(\matr{A}) \nonumber
	\end{equation}
\end{Theorem}
\begin{Theorem}
	For a matrix $ \matr{A} $
	\begin{equation}
		\matr{A}\adj(\matr{A}) = \det(\matr{A})\matr{I} \nonumber
	\end{equation}
\end{Theorem}
\begin{Theorem}
	If $ \matr{A} $ is an invertible $ n \times n $ matrix, then for each $ \vec{b} \in \Real^n $, the equation $ \matr{A}\vec{x}=\vec{b} $ has the unique solution $ \vec{x}=\matr{A}^{-1}\vec{b} $.
\end{Theorem}
\begin{Theorem}
	\begin{itemize}
		\item The inverse of the inverse of a invertible matrix is the matrix itself.
		\item The inverse of the product of arbitrary number of invertible square matrices is the product of the inverse of themselves multiplied in the reverse order.
		\item The transpose of a invertible matrix is also invertible. Moreover, the inverse of a matrix's transpose is the transpose of the matrix's inverse.
	\end{itemize}
\end{Theorem}
\begin{Definition}
	An \textit{elementary matrix} is a matrix obtained by performing a single elementary row operation on a identity matrix.
\end{Definition}
\begin{Theorem}
	If an elementary row operations is performed on an $ m \times n $ matrix $ \matr{A} $, the resulting matrix can be written as $ \matr{EA} $, where the $ m \times m $ matrix $ \matr{E} $ is created by performing the same row operation on $ \matr{I}_m $.
\end{Theorem}
\begin{Theorem}
	Each elementary matrix $ \matr{E} $ is invertible. The inverse of $ \matr{E} $ is the elementary matrix of the same type that transforms $ \matr{E} $ back into $ \matr{I} $.
\end{Theorem}
\begin{Theorem}
	An $ n \times n $ matrix $ \matr{A} $ is invertible iff $ \matr{A} $ is a row equivalent to $ \matr{I}_n $, and in this case, any sequence of elementary row operations that reduces $ \matr{A} $ to $ \matr{I}_n $ also transforms $ \matr{I}_n $ into $ \matr{A}^{-1} $.
\end{Theorem}

	\chapter{Vector Spaces and Subspaces}
	\chapter{Orthogonality}
	\chapter{Determinants}
	\chapter{Eigenvalues and Eigenvectors}
	\chapter{The Singular Value Decomposition(SVD)}
	\chapter{Linear Transformations}
	\chapter{Complex Vectors and Matrices}
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
\end{document}